{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2532e053",
   "metadata": {},
   "source": [
    "# Chapter 7: Building Custom Object Detectors\n",
    "\n",
    "This Jupyter Notebook allows you to interactively edit and run a subset of the code samples from the corresponding chapter in our book, *Learning OpenCV 5 Computer Vision with Python 3*.\n",
    "\n",
    "Any Jupyter server should be capable of running the Notebook, even if the sample input images files are not available in the server's local filesystem. For example, you can run the Notebook in Google Colab by opening the following link in your Web browser: https://colab.research.google.com/github/PacktPublishing/Learning-OpenCV-5-Computer-Vision-with-Python-Fourth-Edition/blob/main/chapter07/chapter07.ipynb. Specifically, this link opens the Notebook's latest version, hosted on GitHub.\n",
    "\n",
    "For additional code samples and instructions, please refer to the book and to the GitHub repository at https://github.com/PacktPublishing/Learning-OpenCV-5-Computer-Vision-with-Python-Fourth-Edition.\n",
    "\n",
    "## Upgrading OpenCV and running the compatibility script\n",
    "\n",
    "**IMPORTANT:** Run the scripts in this section first and run them in order; otherwise, code in subsequent sections may fail or hang.\n",
    "\n",
    "If you are running this Notebook in Google Colab or another environment where OpenCV might not be up-to-date, run the following command to upgrade the OpenCV pip package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773edec1",
   "metadata": {},
   "source": [
    "If the preceding command's output includes a prompt to restart the kernel, do restart it.\n",
    "\n",
    "Now, run the following script, which provides a compatibility layer between OpenCV and Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af37f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../compat/jupyter_compat.py\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import PIL.Image\n",
    "\n",
    "from IPython import display\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def cv2_imshow(winname, mat):\n",
    "    mat = mat.clip(0, 255).astype('uint8')\n",
    "    if mat.ndim == 3:\n",
    "        if mat.shape[2] == 4:\n",
    "            mat = cv2.cvtColor(mat, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            mat = cv2.cvtColor(mat, cv2.COLOR_BGR2RGB)\n",
    "    display.display(PIL.Image.fromarray(mat))\n",
    "\n",
    "cv2.imshow = cv2_imshow\n",
    "\n",
    "\n",
    "def cv2_waitKey(delay=0):\n",
    "    return -1\n",
    "\n",
    "cv2.waitKey = cv2_waitKey\n",
    "\n",
    "\n",
    "def cv2_imread(filename, flags=cv2.IMREAD_COLOR):\n",
    "    if os.path.exists(filename):\n",
    "        image = cv2._imread(filename, flags)\n",
    "    else:\n",
    "        url = f'https://github.com/PacktPublishing/Learning-OpenCV-5-Computer-Vision-with-Python-Fourth-Edition/raw/main/*/{filename}'\n",
    "        resp = urlopen(url)\n",
    "        image = numpy.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "        image = cv2.imdecode(image, flags)\n",
    "    return image\n",
    "\n",
    "# Cache the original implementation of `imread`, if we have not already\n",
    "# done so on a previous run of this cell.\n",
    "if '_imread' not in dir(cv2):\n",
    "    cv2._imread = cv2.imread\n",
    "\n",
    "cv2.imread = cv2_imread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7748f33",
   "metadata": {},
   "source": [
    "What did we just do? We imported OpenCV and we replaced some of OpenCV's I/O functions with our own functions that do not rely on a windowed environment or on a local filesystem.\n",
    "\n",
    "## Detecting people with HOG descriptors\n",
    "\n",
    "Let's start by detecting people using HOG descriptors.\n",
    "\n",
    "Run the following script, which uses HOG with OpenCV's default person detector to finds people in an image of a hayfield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6366f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load detect_people_hog.py\n",
    "import cv2\n",
    "\n",
    "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
    "OPENCV_MINOR_VERSION = int(cv2.__version__.split('.')[1])\n",
    "\n",
    "def is_inside(i, o):\n",
    "    ix, iy, iw, ih = i\n",
    "    ox, oy, ow, oh = o\n",
    "    return ix > ox and ix + iw < ox + ow and \\\n",
    "        iy > oy and iy + ih < oy + oh\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "img = cv2.imread('../images/haying.jpg')\n",
    "\n",
    "if OPENCV_MAJOR_VERSION >= 5 or \\\n",
    "        (OPENCV_MAJOR_VERSION == 4 and OPENCV_MINOR_VERSION >= 6):\n",
    "    # OpenCV 4.6 or a later version is being used.\n",
    "    found_rects, found_weights = hog.detectMultiScale(\n",
    "        img, winStride=(4, 4), scale=1.02, groupThreshold=1.9)\n",
    "else:\n",
    "    # OpenCV 4.5 or an earlier version is being used.\n",
    "    # The groupThreshold parameter used to be named finalThreshold.\n",
    "    found_rects, found_weights = hog.detectMultiScale(\n",
    "        img, winStride=(4, 4), scale=1.02, finalThreshold=1.9)\n",
    "\n",
    "found_rects_filtered = []\n",
    "found_weights_filtered = []\n",
    "for ri, r in enumerate(found_rects):\n",
    "    for qi, q in enumerate(found_rects):\n",
    "        if ri != qi and is_inside(r, q):\n",
    "            break\n",
    "    else:\n",
    "        found_rects_filtered.append(r)\n",
    "        found_weights_filtered.append(found_weights[ri])\n",
    "\n",
    "for ri, r in enumerate(found_rects_filtered):\n",
    "    x, y, w, h = r\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "    text = '%.2f' % found_weights_filtered[ri]\n",
    "    cv2.putText(img, text, (x, y - 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "cv2.imshow('Women in Hayfield Detected', img)\n",
    "cv2.imwrite('./women_in_hayfield_detected.png', img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef736dc",
   "metadata": {},
   "source": [
    "You probably see that some but not all of the people were detected. You may see false positive detections too. Try fine-tuning the parameters of `detectMultiScale` to see how the detection results are affected.\n",
    "\n",
    "You can also try the following variant of the script, which replaces OpenCV's default person detector with the Daimler person detector and adjusts the parameters of `detectMultiScale`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load detect_people_hog_daimler.py\n",
    "import cv2\n",
    "\n",
    "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
    "OPENCV_MINOR_VERSION = int(cv2.__version__.split('.')[1])\n",
    "\n",
    "def is_inside(i, o):\n",
    "    ix, iy, iw, ih = i\n",
    "    ox, oy, ow, oh = o\n",
    "    return ix > ox and ix + iw < ox + ow and \\\n",
    "        iy > oy and iy + ih < oy + oh\n",
    "\n",
    "hog = cv2.HOGDescriptor((48, 96), (16, 16), (8, 8), (8, 8), 9)\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDaimlerPeopleDetector())\n",
    "\n",
    "img = cv2.imread('../images/haying.jpg')\n",
    "\n",
    "if OPENCV_MAJOR_VERSION >= 5 or \\\n",
    "        (OPENCV_MAJOR_VERSION == 4 and OPENCV_MINOR_VERSION >= 6):\n",
    "    # OpenCV 4.6 or a later version is being used.\n",
    "    found_rects, found_weights = hog.detectMultiScale(\n",
    "        img, winStride=(8, 8), scale=1.04, groupThreshold=6.0)\n",
    "else:\n",
    "    # OpenCV 4.5 or an earlier version is being used.\n",
    "    # The groupThreshold parameter used to be named finalThreshold.\n",
    "    found_rects, found_weights = hog.detectMultiScale(\n",
    "        img, winStride=(8, 8), scale=1.04, finalThreshold=6.0)\n",
    "\n",
    "found_rects_filtered = []\n",
    "found_weights_filtered = []\n",
    "for ri, r in enumerate(found_rects):\n",
    "    for qi, q in enumerate(found_rects):\n",
    "        if ri != qi and is_inside(r, q):\n",
    "            break\n",
    "    else:\n",
    "        found_rects_filtered.append(r)\n",
    "        found_weights_filtered.append(found_weights[ri])\n",
    "\n",
    "for ri, r in enumerate(found_rects_filtered):\n",
    "    x, y, w, h = r\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "    text = '%.2f' % found_weights_filtered[ri]\n",
    "    cv2.putText(img, text, (x, y - 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "cv2.imshow('Women in Hayfield Detected', img)\n",
    "cv2.imwrite('./women_in_hayfield_detected_daimler.png', img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a9e32",
   "metadata": {},
   "source": [
    "You probably see that the Daimler person detector did slightly better at detecting people in the distance, as its window size (48x96) is smaller than the default person detector's window size (64x128). Again, feel free to experiment with the parameters of `detectMultiScale`.\n",
    "\n",
    "## Classifying cars v. non-cars using SIFT, BoW, and SVM\n",
    "\n",
    "Now, let's train and test a custom image classifier using SIFT descriptors, BoW descriptors, and an SVM classifier.\n",
    "\n",
    "First, we need a set of training images. Run the following commands to download and extract a copy of the UIUC Image Database for Car Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c3e6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget -O CarData.tar.gz https://github.com/gcr/arc-evaluator/raw/master/CarData.tar.gz\n",
    "!tar -xvzf CarData.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acd469",
   "metadata": {},
   "source": [
    "Run the following script to train and test the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load detect_car_bow_svm.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('CarData'):\n",
    "    print('CarData folder not found. Please download and unzip '\n",
    "          'https://github.com/gcr/arc-evaluator/raw/master/CarData.tar.gz '\n",
    "          'into the same folder as this script.')\n",
    "    exit(1)\n",
    "\n",
    "BOW_NUM_TRAINING_SAMPLES_PER_CLASS = 10\n",
    "SVM_NUM_TRAINING_SAMPLES_PER_CLASS = 110\n",
    "\n",
    "BOW_NUM_CLUSTERS = 40\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "bow_kmeans_trainer = cv2.BOWKMeansTrainer(BOW_NUM_CLUSTERS)\n",
    "bow_extractor = cv2.BOWImgDescriptorExtractor(sift, flann)\n",
    "\n",
    "def get_pos_and_neg_paths(i):\n",
    "    pos_path = 'CarData/TrainImages/pos-%d.pgm' % (i+1)\n",
    "    neg_path = 'CarData/TrainImages/neg-%d.pgm' % (i+1)\n",
    "    return pos_path, neg_path\n",
    "\n",
    "def add_sample(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    if descriptors is not None:\n",
    "        bow_kmeans_trainer.add(descriptors)\n",
    "\n",
    "for i in range(BOW_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
    "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
    "    add_sample(pos_path)\n",
    "    add_sample(neg_path)\n",
    "\n",
    "voc = bow_kmeans_trainer.cluster()\n",
    "bow_extractor.setVocabulary(voc)\n",
    "\n",
    "def extract_bow_descriptors(img):\n",
    "    features = sift.detect(img)\n",
    "    return bow_extractor.compute(img, features)\n",
    "\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for i in range(SVM_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
    "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
    "    pos_img = cv2.imread(pos_path, cv2.IMREAD_GRAYSCALE)\n",
    "    pos_descriptors = extract_bow_descriptors(pos_img)\n",
    "    if pos_descriptors is not None:\n",
    "        training_data.extend(pos_descriptors)\n",
    "        training_labels.append(1)\n",
    "    neg_img = cv2.imread(neg_path, cv2.IMREAD_GRAYSCALE)\n",
    "    neg_descriptors = extract_bow_descriptors(neg_img)\n",
    "    if neg_descriptors is not None:\n",
    "        training_data.extend(neg_descriptors)\n",
    "        training_labels.append(-1)\n",
    "\n",
    "svm = cv2.ml.SVM_create()\n",
    "\n",
    "svm.train(np.array(training_data), cv2.ml.ROW_SAMPLE,\n",
    "          np.array(training_labels))\n",
    "\n",
    "for test_img_path in ['CarData/TestImages/test-0.pgm',\n",
    "                      'CarData/TestImages/test-1.pgm',\n",
    "                      '../images/car.jpg',\n",
    "                      '../images/haying.jpg',\n",
    "                      '../images/statue.jpg',\n",
    "                      '../images/woodcutters.jpg']:\n",
    "    img = cv2.imread(test_img_path)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    descriptors = extract_bow_descriptors(gray_img)\n",
    "    prediction = svm.predict(descriptors)\n",
    "    if prediction[1][0][0] == 1.0:\n",
    "        text = 'car'\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        text = 'not car'\n",
    "        color = (0, 0, 255)\n",
    "    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                color, 2, cv2.LINE_AA)\n",
    "    cv2.imshow(test_img_path, img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d40390",
   "metadata": {},
   "source": [
    "You probably see that some classification results are correct and others are incorrect. Try fine-tuning the parameters, including the number of training samples and the number of BoW clusters, to see how the classification results are affected.\n",
    "\n",
    "## Using a sliding window and NMS to detect cars' positions\n",
    "\n",
    "Building on the previous section's sample, we are going to use our classifier in combination with a sliding window approach and non-maximum suppression (NMS) in order to detect the positions of cars in test images.\n",
    "\n",
    "First, run the following script, which defines a function to filters a list of rectangles based on NMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19122723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load non_max_suppression.py\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "\n",
    "# Malisiewicz et al.\n",
    "# Python port by Adrian Rosebrock\n",
    "# https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # initialize the list of picked indexes \n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    scores = boxes[:,4]\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the score/probability of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(scores)[::-1]\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked\n",
    "    return boxes[pick]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26218b8",
   "metadata": {},
   "source": [
    "The preceding script does not (on its own) produce any output but it does define an NMS function that we will import and use.\n",
    "\n",
    "Run the following script, which trains another car classifier and then detects cars in test images using a sliding window and NMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load detect_car_bow_svm_sliding_window.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# When running in Jupyter, the `non_max_suppression_fast` function should\n",
    "# already be in the global scope. Otherwise, import it now.\n",
    "if 'non_max_suppression_fast' not in globals():\n",
    "    from non_max_suppression import non_max_suppression_fast\n",
    "\n",
    "if not os.path.isdir('CarData'):\n",
    "    print('CarData folder not found. Please download and unzip '\n",
    "          'https://github.com/gcr/arc-evaluator/raw/master/CarData.tar.gz '\n",
    "          'into the same folder as this script.')\n",
    "    exit(1)\n",
    "\n",
    "BOW_NUM_TRAINING_SAMPLES_PER_CLASS = 10\n",
    "SVM_NUM_TRAINING_SAMPLES_PER_CLASS = 110\n",
    "\n",
    "BOW_NUM_CLUSTERS = 12\n",
    "SVM_SCORE_THRESHOLD = 2.2\n",
    "NMS_OVERLAP_THRESHOLD = 0.4\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "bow_kmeans_trainer = cv2.BOWKMeansTrainer(BOW_NUM_CLUSTERS)\n",
    "bow_extractor = cv2.BOWImgDescriptorExtractor(sift, flann)\n",
    "\n",
    "def get_pos_and_neg_paths(i):\n",
    "    pos_path = 'CarData/TrainImages/pos-%d.pgm' % (i+1)\n",
    "    neg_path = 'CarData/TrainImages/neg-%d.pgm' % (i+1)\n",
    "    return pos_path, neg_path\n",
    "\n",
    "def add_sample(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    if descriptors is not None:\n",
    "        bow_kmeans_trainer.add(descriptors)\n",
    "\n",
    "for i in range(BOW_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
    "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
    "    add_sample(pos_path)\n",
    "    add_sample(neg_path)\n",
    "\n",
    "voc = bow_kmeans_trainer.cluster()\n",
    "bow_extractor.setVocabulary(voc)\n",
    "\n",
    "def extract_bow_descriptors(img):\n",
    "    features = sift.detect(img)\n",
    "    return bow_extractor.compute(img, features)\n",
    "\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for i in range(SVM_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
    "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
    "    pos_img = cv2.imread(pos_path, cv2.IMREAD_GRAYSCALE)\n",
    "    pos_descriptors = extract_bow_descriptors(pos_img)\n",
    "    if pos_descriptors is not None:\n",
    "        training_data.extend(pos_descriptors)\n",
    "        training_labels.append(1)\n",
    "    neg_img = cv2.imread(neg_path, cv2.IMREAD_GRAYSCALE)\n",
    "    neg_descriptors = extract_bow_descriptors(neg_img)\n",
    "    if neg_descriptors is not None:\n",
    "        training_data.extend(neg_descriptors)\n",
    "        training_labels.append(-1)\n",
    "\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setType(cv2.ml.SVM_C_SVC)\n",
    "svm.setC(50)\n",
    "\n",
    "svm.train(np.array(training_data), cv2.ml.ROW_SAMPLE,\n",
    "          np.array(training_labels))\n",
    "\n",
    "def pyramid(img, scale_factor=1.05, min_size=(100, 40),\n",
    "            max_size=(600, 240)):\n",
    "    h, w = img.shape\n",
    "    min_w, min_h = min_size\n",
    "    max_w, max_h = max_size\n",
    "    while w >= min_w and h >= min_h:\n",
    "        if w <= max_w and h <= max_h:\n",
    "            yield img\n",
    "        w /= scale_factor\n",
    "        h /= scale_factor\n",
    "        img = cv2.resize(img, (int(w), int(h)),\n",
    "                         interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def sliding_window(img, step=20, window_size=(100, 40)):\n",
    "    img_h, img_w = img.shape\n",
    "    window_w, window_h = window_size\n",
    "    for y in range(0, img_w, step):\n",
    "        for x in range(0, img_h, step):\n",
    "            roi = img[y:y+window_h, x:x+window_w]\n",
    "            roi_h, roi_w = roi.shape\n",
    "            if roi_w == window_w and roi_h == window_h:\n",
    "                yield (x, y, roi)\n",
    "\n",
    "for test_img_path in ['CarData/TestImages/test-0.pgm',\n",
    "                      'CarData/TestImages/test-1.pgm',\n",
    "                      '../images/car.jpg',\n",
    "                      '../images/haying.jpg',\n",
    "                      '../images/statue.jpg',\n",
    "                      '../images/woodcutters.jpg']:\n",
    "    img = cv2.imread(test_img_path)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    pos_rects = []\n",
    "    for resized in pyramid(gray_img):\n",
    "        for x, y, roi in sliding_window(resized):\n",
    "            descriptors = extract_bow_descriptors(roi)\n",
    "            if descriptors is None:\n",
    "                continue\n",
    "            prediction = svm.predict(descriptors)\n",
    "            if prediction[1][0][0] == 1.0:\n",
    "                raw_prediction = svm.predict(\n",
    "                    descriptors, flags=cv2.ml.STAT_MODEL_RAW_OUTPUT)\n",
    "                score = -raw_prediction[1][0][0]\n",
    "                if score > SVM_SCORE_THRESHOLD:\n",
    "                    h, w = roi.shape\n",
    "                    scale = gray_img.shape[0] / float(resized.shape[0])\n",
    "                    pos_rects.append([int(x * scale),\n",
    "                                      int(y * scale),\n",
    "                                      int((x+w) * scale),\n",
    "                                      int((y+h) * scale),\n",
    "                                      score])\n",
    "    pos_rects = non_max_suppression_fast(\n",
    "        np.array(pos_rects), NMS_OVERLAP_THRESHOLD)\n",
    "    for x0, y0, x1, y1, score in pos_rects:\n",
    "        cv2.rectangle(img, (int(x0), int(y0)), (int(x1), int(y1)),\n",
    "                      (0, 255, 255), 2)\n",
    "        text = '%.2f' % score\n",
    "        cv2.putText(img, text, (int(x0), int(y0) - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "    cv2.imshow(test_img_path, img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c9907",
   "metadata": {},
   "source": [
    "You probably see that some detection results are correct and others are incorrect. Try fine-tuning the parameters, including the number of training samples, the number of BoW clusters, and the `pyramid` and `sliding_window` parameters, to see how the detection results are affected.\n",
    "\n",
    "## Training a custom HOG model to detect cars' positions\n",
    "\n",
    "The previous section's exercise may have left you wondering whether we can achieve better results by training our own HOG detector for cars. Indeed, we can and we will!\n",
    "\n",
    "Using the same car database as our previous two samples, the following script extracts HOG features from the training images, uses the HOG features to train an SVM classifier, and builds a HOG detector around the SVM classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605744ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load detect_car_hog_svm.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
    "OPENCV_MINOR_VERSION = int(cv2.__version__.split('.')[1])\n",
    "\n",
    "if not os.path.isdir('CarData'):\n",
    "    print('CarData folder not found. Please download and unzip '\n",
    "          'https://github.com/gcr/arc-evaluator/raw/master/CarData.tar.gz '\n",
    "          'into the same folder as this script.')\n",
    "    exit(1)\n",
    "\n",
    "HOG_WINDOW_SIZE = (96, 48)\n",
    "HOG_WEIGHT_THRESHOLD = 0.45\n",
    "\n",
    "SVM_NUM_TRAINING_SAMPLES_PER_CLASS = 300\n",
    "\n",
    "hog = cv2.HOGDescriptor(HOG_WINDOW_SIZE, (16, 16), (8, 8), (8, 8), 9)\n",
    "\n",
    "def get_pos_and_neg_paths(i):\n",
    "    pos_path = 'CarData/TrainImages/pos-%d.pgm' % (i+1)\n",
    "    neg_path = 'CarData/TrainImages/neg-%d.pgm' % (i+1)\n",
    "    return pos_path, neg_path\n",
    "\n",
    "def extract_hog_descriptors(img):\n",
    "    resized = cv2.resize(img, HOG_WINDOW_SIZE, cv2.INTER_CUBIC)\n",
    "    return hog.compute(resized, (16, 16), (0, 0))\n",
    "\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for i in range(SVM_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
    "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
    "    pos_img = cv2.imread(pos_path, cv2.IMREAD_GRAYSCALE)\n",
    "    pos_descriptors = extract_hog_descriptors(pos_img)\n",
    "    if pos_descriptors is not None:\n",
    "        training_data.append(pos_descriptors)\n",
    "        training_labels.append(1)\n",
    "    neg_img = cv2.imread(neg_path, cv2.IMREAD_GRAYSCALE)\n",
    "    neg_descriptors = extract_hog_descriptors(neg_img)\n",
    "    if neg_descriptors is not None:\n",
    "        training_data.append(neg_descriptors)\n",
    "        training_labels.append(-1)\n",
    "\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setDegree(3)\n",
    "criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 1000, 1e-3)\n",
    "svm.setTermCriteria(criteria)\n",
    "svm.setKernel(cv2.ml.SVM_LINEAR)\n",
    "svm.setNu(0.5)\n",
    "svm.setP(0.1)\n",
    "svm.setC(0.01)\n",
    "svm.setType(cv2.ml.SVM_EPS_SVR)\n",
    "\n",
    "svm.train(np.array(training_data), cv2.ml.ROW_SAMPLE,\n",
    "          np.array(training_labels))\n",
    "\n",
    "support_vectors = np.transpose(svm.getSupportVectors())\n",
    "rho, _, _ = svm.getDecisionFunction(0)\n",
    "svm_detector = np.append(support_vectors, [[-rho]], 0)\n",
    "hog.setSVMDetector(svm_detector)\n",
    "\n",
    "def is_inside(i, o):\n",
    "    ix, iy, iw, ih = i\n",
    "    ox, oy, ow, oh = o\n",
    "    return ix > ox and ix + iw < ox + ow and \\\n",
    "        iy > oy and iy + ih < oy + oh\n",
    "\n",
    "for test_img_path in ['CarData/TestImages/test-0.pgm',\n",
    "                      'CarData/TestImages/test-1.pgm',\n",
    "                      '../images/car.jpg',\n",
    "                      '../images/haying.jpg',\n",
    "                      '../images/statue.jpg',\n",
    "                      '../images/woodcutters.jpg']:\n",
    "    img = cv2.imread(test_img_path)\n",
    "\n",
    "    if OPENCV_MAJOR_VERSION >= 5 or \\\n",
    "            (OPENCV_MAJOR_VERSION == 4 and OPENCV_MINOR_VERSION >= 6):\n",
    "        # OpenCV 4.6 or a later version is being used.\n",
    "        found_rects, found_weights = hog.detectMultiScale(\n",
    "            img, winStride=(8, 8), scale=1.03, groupThreshold=2.0)\n",
    "    else:\n",
    "        # OpenCV 4.5 or an earlier version is being used.\n",
    "        # The groupThreshold parameter used to be named finalThreshold.\n",
    "        found_rects, found_weights = hog.detectMultiScale(\n",
    "            img, winStride=(8, 8), scale=1.03, finalThreshold=2.0)\n",
    "\n",
    "    found_rects_filtered = []\n",
    "    found_weights_filtered = []\n",
    "    for ri, r in enumerate(found_rects):\n",
    "        if found_weights[ri] < HOG_WEIGHT_THRESHOLD:\n",
    "            continue\n",
    "        for qi, q in enumerate(found_rects):\n",
    "            if found_weights[qi] < HOG_WEIGHT_THRESHOLD:\n",
    "                continue\n",
    "            if ri != qi and is_inside(r, q):\n",
    "                break\n",
    "        else:\n",
    "            found_rects_filtered.append(r)\n",
    "            found_weights_filtered.append(found_weights[ri])\n",
    "\n",
    "    for ri, r in enumerate(found_rects_filtered):\n",
    "        x, y, w, h = r\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "        text = '%.2f' % found_weights_filtered[ri]\n",
    "        cv2.putText(img, text, (x, y - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(test_img_path, img)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77132e7f",
   "metadata": {},
   "source": [
    "Note that a sliding window approach is built into the `detectMultiScale` method of `cv2.HOGDescriptor`, so we do not need a custom implementation of sliding windows or NMS in this case.\n",
    "\n",
    "You should see that the detection results are much more reliable in this latest sample. Try fine-tuning the parameters, including the number of training samples, the window size, the HOG weight threshold, and the parameters of `detectMultiScale`, to see how the detection results are affected.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "That is all for now! Please refer to the book for additional details on these samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2532e053",
   "metadata": {},
   "source": [
    "# Chapter 6: Retrieving Images and Searching Using Image Descriptors\n",
    "\n",
    "This Jupyter Notebook allows you to interactively edit and run a subset of the code samples from the corresponding chapter in our book, *Learning OpenCV 5 Computer Vision with Python 3*.\n",
    "\n",
    "Any Jupyter server should be capable of running the Notebook, even if the sample input images files are not available in the server's local filesystem. For example, you can run the Notebook in Google Colab by opening the following link in your Web browser: https://colab.research.google.com/github/PacktPublishing/Learning-OpenCV-5-Computer-Vision-with-Python-Fourth-Edition/blob/main/chapter06/chapter06.ipynb. Specifically, this link opens the Notebook's latest version, hosted on GitHub.\n",
    "\n",
    "For additional code samples and instructions, please refer to the book and to the GitHub repository at https://github.com/PacktPublishing/Learning-OpenCV-5-Computer-Vision-with-Python-Fourth-Edition.\n",
    "\n",
    "## Upgrading OpenCV and running the compatibility script\n",
    "\n",
    "**IMPORTANT:** Run the scripts in this section first and run them in order; otherwise, code in subsequent sections may fail or hang.\n",
    "\n",
    "If you are running this Notebook in Google Colab or another environment where OpenCV might not be up-to-date, run the following command to upgrade the OpenCV pip package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773edec1",
   "metadata": {},
   "source": [
    "If the preceding command's output includes a prompt to restart the kernel, do restart it.\n",
    "\n",
    "Now, run the following script, which provides a compatibility layer between OpenCV and Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af37f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../compat/jupyter_compat.py\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import PIL.Image\n",
    "\n",
    "from IPython import display\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def cv2_imshow(winname, mat):\n",
    "    mat = mat.clip(0, 255).astype('uint8')\n",
    "    if mat.ndim == 3:\n",
    "        if mat.shape[2] == 4:\n",
    "            mat = cv2.cvtColor(mat, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            mat = cv2.cvtColor(mat, cv2.COLOR_BGR2RGB)\n",
    "    display.display(PIL.Image.fromarray(mat))\n",
    "\n",
    "cv2.imshow = cv2_imshow\n",
    "\n",
    "\n",
    "def cv2_waitKey(delay=0):\n",
    "    return -1\n",
    "\n",
    "cv2.waitKey = cv2_waitKey\n",
    "\n",
    "\n",
    "def cv2_imread(filename, flags=cv2.IMREAD_COLOR):\n",
    "    if os.path.exists(filename):\n",
    "        image = cv2._imread(filename, flags)\n",
    "    else:\n",
    "        url = f'https://github.com/PacktPublishing/Learning-OpenCV-5-Computer-Vision-with-Python-Fourth-Edition/raw/main/*/{filename}'\n",
    "        resp = urlopen(url)\n",
    "        image = numpy.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "        image = cv2.imdecode(image, flags)\n",
    "    return image\n",
    "\n",
    "# Cache the original implementation of `imread`, if we have not already\n",
    "# done so on a previous run of this cell.\n",
    "if '_imread' not in dir(cv2):\n",
    "    cv2._imread = cv2.imread\n",
    "\n",
    "cv2.imread = cv2_imread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7748f33",
   "metadata": {},
   "source": [
    "What did we just do? We imported OpenCV and we replaced some of OpenCV's I/O functions with our own functions that do not rely on a windowed environment or on a local filesystem.\n",
    "\n",
    "## Detecting Harris corners\n",
    "\n",
    "Let's start by finding corners using the Harris corner detection algorithm.\n",
    "\n",
    "Run the following script, which finds corners in an image of a chessboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6366f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load corner.py\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('../images/chess_board.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "dst = cv2.cornerHarris(gray, 2, 23, 0.04)\n",
    "img[dst > 0.01 * dst.max()] = [0, 0, 255]\n",
    "cv2.imshow('corners', img)\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef736dc",
   "metadata": {},
   "source": [
    "You probably see that the corners of the squares were detected but many other corners were detected too. Try fine-tuning the parameters of `cornerHarris` to see how the detection results are affected.\n",
    "\n",
    "## Detecting DoG features and extracting SIFT descriptors\n",
    "\n",
    "Now, let's experiment with detecting keypoints (specifically, DoG features) and computing keypoint descriptors (specifically, SIFT descriptors).\n",
    "\n",
    "Run the following script, which performs keypoint detection and description for an image of the beautiful landscape at Varese, Lombady, Italy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sift.py\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('../images/varese.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "cv2.drawKeypoints(img, keypoints, img, (51, 163, 236),\n",
    "                  cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('sift_keypoints', img)\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d40390",
   "metadata": {},
   "source": [
    "The visualization shows the scale and orientation of the various SIFT descriptors.\n",
    "\n",
    "## Using ORB descriptors and brute-force matching\n",
    "\n",
    "We can apply a brute-force approach to the problem of comparing keypoint descriptors in two images. Thereby, we can find matching keypoints. Let's do so for two images that we describe using the ORB algorithm.\n",
    "\n",
    "Run the following script, which attempts to find matches between an image of the NASA logo and an image of the Kennedy Space Center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19122723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load orb.py\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the images.\n",
    "img0 = cv2.imread('../images/nasa_logo.png',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread('../images/kennedy_space_center.jpg',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform ORB feature detection and description.\n",
    "orb = cv2.ORB_create()\n",
    "kp0, des0 = orb.detectAndCompute(img0, None)\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "\n",
    "# Perform brute-force matching.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des0, des1)\n",
    "\n",
    "# Sort the matches by distance.\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "\n",
    "# Draw the best 25 matches.\n",
    "img_matches = cv2.drawMatches(\n",
    "    img0, kp0, img1, kp1, matches[:25], img1,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Show the matches.\n",
    "plt.imshow(img_matches)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26218b8",
   "metadata": {},
   "source": [
    "Most of those matches are clearly false positives! We will improve on the result in the next section.\n",
    "\n",
    "## Filtering matches using K-Nearest Neighbors and the ratio test\n",
    "\n",
    "We can eliminate many false positive matches by performing KNN matching with the ratio test (whereby we reject a dubious \"best\" match if it is not much better than a \"second-best\" match).\n",
    "\n",
    "Run the following script, which first computes a large set of matches and then filters them, again using the NASA logo and Kennedy Space Center as subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load orb_knn.py\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the images.\n",
    "img0 = cv2.imread('../images/nasa_logo.png',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread('../images/kennedy_space_center.jpg',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform ORB feature detection and description.\n",
    "orb = cv2.ORB_create()\n",
    "kp0, des0 = orb.detectAndCompute(img0, None)\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "\n",
    "# Perform brute-force KNN matching.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "pairs_of_matches = bf.knnMatch(des0, des1, k=2)\n",
    "\n",
    "# Sort the pairs of matches by distance.\n",
    "pairs_of_matches = sorted(pairs_of_matches, key=lambda x:x[0].distance)\n",
    "\n",
    "# Draw the 25 best pairs of matches.\n",
    "img_pairs_of_matches = cv2.drawMatchesKnn(\n",
    "    img0, kp0, img1, kp1, pairs_of_matches[:25], img1,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Show the pairs of matches.\n",
    "plt.imshow(img_pairs_of_matches)\n",
    "plt.show()\n",
    "\n",
    "# Apply the ratio test.\n",
    "matches = [x[0] for x in pairs_of_matches\n",
    "           if len(x) > 1 and x[0].distance < 0.8 * x[1].distance]\n",
    "\n",
    "# Draw the best 25 matches.\n",
    "img_matches = cv2.drawMatches(\n",
    "    img0, kp0, img1, kp1, matches[:25], img1,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Show the matches.\n",
    "plt.imshow(img_matches)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb012266",
   "metadata": {},
   "source": [
    "Try adjusting the value of the ratio test threshold (from `0.8` to some other value in the range `(0.0, 1.0)`) to see how the results are affected.\n",
    "\n",
    "## Matching with FLANN\n",
    "\n",
    "Similarly, we can combine KNN matching and the ratio test with FLANN-based matching instead of brute-force matching.\n",
    "\n",
    "Run the following script, which finds matches between images of Gaugin paintings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load flann.py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img0 = cv2.imread('../images/gauguin_entre_les_lys.jpg',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread('../images/gauguin_paintings.png',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform SIFT feature detection and description.\n",
    "sift = cv2.SIFT_create()\n",
    "kp0, des0 = sift.detectAndCompute(img0, None)\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "\n",
    "# Define FLANN-based matching parameters.\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "# Perform FLANN-based matching.\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des0, des1, k=2)\n",
    "\n",
    "# Prepare an empty mask to draw good matches.\n",
    "mask_matches = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "# Populate the mask based on David G. Lowe's ratio test.\n",
    "for i, (m, n) in enumerate(matches):\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        mask_matches[i]=[1, 0]\n",
    "\n",
    "# Draw the matches that passed the ratio test.\n",
    "img_matches = cv2.drawMatchesKnn(\n",
    "    img0, kp0, img1, kp1, matches, None,\n",
    "    matchColor=(0, 255, 0), singlePointColor=(255, 0, 0),\n",
    "    matchesMask=mask_matches, flags=0)\n",
    "\n",
    "# Show the matches.\n",
    "plt.imshow(img_matches)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ecf52",
   "metadata": {},
   "source": [
    "Try adjusting the FLANN parameters in `index_params` and `search_params` to see how the results are affected.\n",
    "\n",
    "## Finding homography with FLANN-based matches\n",
    "\n",
    "Let's explore the perspective relationship between two matching images (or matching parts of images) by finding the homography between the matching keypoints.\n",
    "\n",
    "Run the following script, which finds the homography between a small image of a tattoo and a larger image of a hand bearing the same tattoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load homography.py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MIN_NUM_GOOD_MATCHES = 10\n",
    "\n",
    "img0 = cv2.imread('../images/tattoos/query.png',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread('../images/tattoos/anchor-man.png',\n",
    "                  cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform SIFT feature detection and description.\n",
    "sift = cv2.SIFT_create()\n",
    "kp0, des0 = sift.detectAndCompute(img0, None)\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "\n",
    "# Define FLANN-based matching parameters.\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "# Perform FLANN-based matching.\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des0, des1, k=2)\n",
    "\n",
    "# Find all the good matches as per Lowe's ratio test.\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "if len(good_matches) >= MIN_NUM_GOOD_MATCHES:\n",
    "    src_pts = np.float32(\n",
    "        [kp0[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32(\n",
    "        [kp1[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    mask_matches = mask.ravel().tolist()\n",
    "\n",
    "    h, w = img0.shape\n",
    "    src_corners = np.float32(\n",
    "        [[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "    dst_corners = cv2.perspectiveTransform(src_corners, M)\n",
    "    dst_corners = dst_corners.astype(np.int32)\n",
    "\n",
    "    # Draw the bounds of the matched region based on the homography.\n",
    "    num_corners = len(dst_corners)\n",
    "    for i in range(num_corners):\n",
    "        x0, y0 = dst_corners[i][0]\n",
    "        if i == num_corners - 1:\n",
    "            next_i = 0\n",
    "        else:\n",
    "            next_i = i + 1\n",
    "        x1, y1 = dst_corners[next_i][0]\n",
    "        cv2.line(img1, (x0, y0), (x1, y1), 255, 3, cv2.LINE_AA)\n",
    "\n",
    "    # Draw the matches that passed the ratio test.\n",
    "    img_matches = cv2.drawMatches(\n",
    "        img0, kp0, img1, kp1, good_matches, None,\n",
    "        matchColor=(0, 255, 0), singlePointColor=None,\n",
    "        matchesMask=mask_matches, flags=2)\n",
    "\n",
    "    # Show the homography and good matches.\n",
    "    plt.imshow(img_matches)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough matches good were found - %d/%d\" % \\\n",
    "          (len(good_matches), MIN_MATCH_COUNT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c9907",
   "metadata": {},
   "source": [
    "Try adjusting the parameters of `findHomography` to see how the results are affected.\n",
    "\n",
    "## Summary\n",
    "\n",
    "That is all for now! Please refer to the book and to the GitHub repository for additional samples involving keypoint descriptors (including the SURF algorithm) and searching for matching images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
